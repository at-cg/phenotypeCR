{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from typing import Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_hpo_output(text):\n",
    "    text = text.replace(\"END\", \"\").strip()  # Remove END marker\n",
    "    terms = text.split(\"\\n\")  # Split lines in the ground truth\n",
    "    parsed = []\n",
    "    for term in terms:\n",
    "        term = term.strip()\n",
    "        if \"|\" in term:  # Process only lines with HPO terms\n",
    "            name, hpo_id = term.split(\"|\")\n",
    "            name = name.strip().lower()  # Normalize name\n",
    "            hpo_id = hpo_id.strip().upper()  # Normalize ID\n",
    "            parsed.append((name, hpo_id))\n",
    "    return set(parsed)\n",
    "def parse_gpt_output(text):\n",
    "    text = text.strip()\n",
    "    terms = text.split(\",\")  # Split by commas for predictions\n",
    "    parsed = []\n",
    "    for term in terms:\n",
    "        term = term.strip()\n",
    "        if \"(\" in term and \")\" in term:\n",
    "            # Split only on the first \"(\" to avoid unpacking issues\n",
    "            name, hpo_id = term.split(\"(\", 1)\n",
    "            parsed.append((name.strip().lower(), hpo_id.strip(\")\").strip().upper()))\n",
    "    return set(parsed)\n",
    "\n",
    "def hp_ids(val_eval_data, gpt_val_data, bool=1):\n",
    "    val_ids, gpt_ids = [], []\n",
    "\n",
    "    for truth, pred in zip(val_eval_data, gpt_val_data):\n",
    "        # Parse ground truth and predictions\n",
    "        true_hpo = parse_hpo_output(truth)\n",
    "        true_hpo = [hp for _, hp in true_hpo]\n",
    "\n",
    "        if bool:\n",
    "            post_hpo = parse_gpt_output(pred)\n",
    "            pred_hpo = [hp for _, hp in post_hpo]\n",
    "        else:\n",
    "            pred_hpo = set(pred)\n",
    "            pred_hpo = [word for entry in pred_hpo for word in entry.split() if word.startswith(\"HP:\")]\n",
    "            pred_hpo = [item.replace(':', '_') for item in pred_hpo]\n",
    "\n",
    "        val_ids.append(true_hpo)\n",
    "        gpt_ids.append(pred_hpo)\n",
    "\n",
    "    return val_ids, gpt_ids\n",
    "\n",
    "def metric(val_ids,gpt_ids):\n",
    "    # Calculate metrics per sample\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for true_hpo, pred_hpo in zip(val_ids, gpt_ids):\n",
    "        # Calculate TP, FP, FN\n",
    "        true_hpo=set(true_hpo)\n",
    "        pred_hpo=set(pred_hpo)\n",
    "        # print(true_hpo)\n",
    "        # print(pred_hpo)\n",
    "        tp = len(true_hpo & pred_hpo)\n",
    "        fp = len(pred_hpo - true_hpo)\n",
    "        fn = len(true_hpo - pred_hpo)\n",
    "\n",
    "        # Precision, Recall, F1 for this sample\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        # Append metrics for this sample\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Average metrics across all samples\n",
    "    avg_precision = sum(precision_scores) / len(precision_scores)\n",
    "    avg_recall = sum(recall_scores) / len(recall_scores)\n",
    "    avg_f1 = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Total Samples: {len(val_ids)}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "def create_finetune_job(client, training: str, validation: Optional[str] = None, model_name: str = \"gpt-4\", epoch: int = 4):\n",
    "    job_params = {\n",
    "        \"training_file\": training,\n",
    "        \"model\": model_name,\n",
    "        \"hyperparameters\": {\"n_epochs\": epoch, \"batch_size\": 1}\n",
    "    }\n",
    "    if validation:  # Add validation file only if provided\n",
    "        job_params[\"validation_file\"] = validation\n",
    "    \n",
    "    # Create the fine-tuning job\n",
    "    response = client.fine_tuning.jobs.create(**job_params)\n",
    "    return response\n",
    "\n",
    "def load_val_data(val_data_path):\n",
    "    # Load validation data\n",
    "    with open(val_data_path, \"r\") as f:\n",
    "        val_data = json.load(f)\n",
    "\n",
    "    val_eval_data = []  \n",
    "    for i in val_data:\n",
    "        val_eval_data.append(i[\"output\"])\n",
    "    return val_data,val_eval_data\n",
    "\n",
    "def load_gpt_val_data(val_data, client,model_id):\n",
    "    gpt_val_data=[]\n",
    "    # Perform inference and calculate metrics for each sample\n",
    "    for entry in val_data:\n",
    "        # Simulate model prediction by passing input through your GPT model\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_id,  # Replace with your fine-tuned model ID\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an assistant specialized in extracting phenotype terms and their corresponding HPO IDs from clinical notes.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Extract relevant phenotype terms and their HPO IDs from the following clinical note:\\n{entry['input']}\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        gpt_val_data.append(response.choices[0].message.content)\n",
    "    return gpt_val_data\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def get_top_k_similarities(embeddings, query, k):\n",
    "    similarities = np.array([cosine_similarity(query, embedding) for embedding in embeddings])\n",
    "    top_k_indices = similarities.argsort()[-k:][::-1]\n",
    "    return top_k_indices, similarities[top_k_indices]\n",
    "\n",
    "def post_gpt(gpt_val_data): \n",
    "    # Load the HPO dictionary\n",
    "    hpo_dict = {}\n",
    "    with open(\"/2022/hpo2022.txt\",encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if \":H\" in line:  # Only process lines containing ':H'\n",
    "                key, val = line.split(\":H\", 1)  # Split only at the first ':H'\n",
    "                val = val.rstrip()  # Remove newline characters\n",
    "                hpo_dict[key] = \"H\" + val\n",
    "\n",
    "    # Load embeddings from CSV\n",
    "    df = pd.read_csv('/2022/HPO_embeddings_38k.csv')\n",
    "    db = df.to_numpy()\n",
    "\n",
    "    # Parse GPT predictions\n",
    "    r = []\n",
    "    for pred in gpt_val_data:\n",
    "        pred_hpo = parse_gpt_output(pred)\n",
    "        r.append(pred_hpo)\n",
    "\n",
    "    # Prepare GPT embeddings\n",
    "    gpt_embeddings = []\n",
    "    for i in r:\n",
    "        temp = []\n",
    "        for j in i:\n",
    "            temp.append(j)\n",
    "        gpt_embeddings.append(list(set(temp)))\n",
    "\n",
    "    gpt_embeddings = [[term[0] for term in sublist if len(term[0]) > 3] for sublist in gpt_embeddings]\n",
    "\n",
    "    # Calculate total number of GPT embeddings\n",
    "    gpt_sum = 0\n",
    "    for i in gpt_embeddings:\n",
    "        gpt_sum += len(i)\n",
    "\n",
    "    # Initialize embedding matrix for GPT\n",
    "    gpt_embeddings_197 = np.zeros((gpt_sum, 1536))\n",
    "    g = 0\n",
    "\n",
    "    # Create embeddings for GPT data\n",
    "    for i in range(len(gpt_embeddings)):\n",
    "        for j in range(len(gpt_embeddings[i])):\n",
    "            response = client.embeddings.create(\n",
    "                input=gpt_embeddings[i][j],\n",
    "                model=\"text-embedding-3-small\"\n",
    "            )\n",
    "            gpt_embeddings_197[g] = response.data[0].embedding\n",
    "            g += 1\n",
    "            if g % 50 == 0:\n",
    "                print(g)\n",
    "\n",
    "    print(\"-----------------indices-----------------\")\n",
    "\n",
    "    # Get top indices for GPT embeddings\n",
    "    top_gpt_indices = []\n",
    "    for i in range(gpt_sum):\n",
    "        top_gpt_indices.append(get_top_k_similarities(db, gpt_embeddings_197[i], 1)[0][0])\n",
    "        if i % 50 == 0:\n",
    "            print(i)\n",
    "\n",
    "    # Retrieve GPT terms based on top indices\n",
    "    gpt_retrieved_terms = []\n",
    "    for i in top_gpt_indices:\n",
    "        gpt_retrieved_terms.append(list(hpo_dict.keys())[i] + \" \" + list(hpo_dict.values())[i])\n",
    "\n",
    "    # Separate GPT terms based on counts\n",
    "    gpt_no_ids = []\n",
    "    for i in gpt_embeddings:\n",
    "        gpt_no_ids.append(len(i))\n",
    "\n",
    "    gpt_sep = []\n",
    "    gpt_i = 0\n",
    "    for i in gpt_no_ids:\n",
    "        gpt_sep.append(list(set(gpt_retrieved_terms[gpt_i:gpt_i + i])))\n",
    "        gpt_i += i\n",
    "\n",
    "    return gpt_sep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biolark-GSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples: 23\n",
      "Average Precision: 0.9588\n",
      "Average Recall: 0.9578\n",
      "Average F1 Score: 0.9581\n",
      "Post processing: embeddings\n",
      "50\n",
      "100\n",
      "150\n",
      "-----------------indices-----------------\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "Total Samples: 23\n",
      "Average Precision: 0.6725\n",
      "Average Recall: 0.6541\n",
      "Average F1 Score: 0.6626\n"
     ]
    }
   ],
   "source": [
    "val_data_path = \"/Evaluation/datasets/biolark_val.json\"\n",
    "model=\"ft:gpt-4o-mini-2024-07-18:iisc-bangalore::AYf5TC9S\"\n",
    "val_data, eval_data = load_val_data(val_data_path)\n",
    "gpt_val_data = load_gpt_val_data(val_data, client, model)\n",
    "val_ids, gpt_ids = hp_ids(eval_data, gpt_val_data)\n",
    "precision, recall, f1 = metric(val_ids, gpt_ids)\n",
    "print(\"Post processing: embeddings\")\n",
    "gpt_sep = post_gpt(gpt_val_data)\n",
    "val_ids, gpt_ids = hp_ids(eval_data, gpt_sep, 0)\n",
    "precision, recall, f1 = metric(val_ids, gpt_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples: 68\n",
      "Average Precision: 0.1134\n",
      "Average Recall: 0.1464\n",
      "Average F1 Score: 0.1247\n",
      "Post processing: embeddings\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "-----------------indices-----------------\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "Total Samples: 68\n",
      "Average Precision: 0.6173\n",
      "Average Recall: 0.7275\n",
      "Average F1 Score: 0.6503\n"
     ]
    }
   ],
   "source": [
    "val_data_path = \"/Evaluation/datasets/ID_68.json\"\n",
    "model=\"ft:gpt-4o-mini-2024-07-18:iisc-bangalore::AYf5TC9S\"\n",
    "val_data, eval_data = load_val_data(val_data_path)\n",
    "gpt_val_data = load_gpt_val_data(val_data, client, model)\n",
    "val_ids, gpt_ids = hp_ids(eval_data, gpt_val_data)\n",
    "precision, recall, f1 = metric(val_ids, gpt_ids)\n",
    "print(\"Post processing: embeddings\")\n",
    "gpt_sep = post_gpt(gpt_val_data)\n",
    "val_ids, gpt_ids = hp_ids(eval_data, gpt_sep, 0)\n",
    "precision, recall, f1 = metric(val_ids, gpt_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
